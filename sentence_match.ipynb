{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_match.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOUHWmkLoc22hdr9BkMs+hT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiel30/Completed/blob/master/sentence_match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wHUEz3gpB4S",
        "outputId": "db3ff32b-9abd-497b-cce9-698ff80d3d47"
      },
      "source": [
        "import torch\n",
        "import datasets\n",
        "import tensorflow as tf\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLkAsOa9r-16",
        "outputId": "25a2036b-4cf3-493b-c62b-9d60c7737fbb"
      },
      "source": [
        "!unzip train_dataset.zip\n",
        "!pip3 install datasets\n",
        "!pip3 install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train_dataset.zip\n",
            "   creating: train_dataset/\n",
            "  inflating: __MACOSX/._train_dataset  \n",
            "  inflating: train_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/train_dataset/._.DS_Store  \n",
            "   creating: train_dataset/BQ/\n",
            "  inflating: __MACOSX/train_dataset/._BQ  \n",
            "   creating: train_dataset/LCQMC/\n",
            "  inflating: __MACOSX/train_dataset/._LCQMC  \n",
            "   creating: train_dataset/OPPO/\n",
            "  inflating: __MACOSX/train_dataset/._OPPO  \n",
            "  inflating: train_dataset/BQ/.DS_Store  \n",
            "  inflating: __MACOSX/train_dataset/BQ/._.DS_Store  \n",
            "  inflating: train_dataset/BQ/dev.txt  \n",
            "  inflating: __MACOSX/train_dataset/BQ/._dev.txt  \n",
            "  inflating: train_dataset/BQ/train.txt  \n",
            "  inflating: __MACOSX/train_dataset/BQ/._train.txt  \n",
            "  inflating: train_dataset/BQ/test.txt  \n",
            "  inflating: __MACOSX/train_dataset/BQ/._test.txt  \n",
            "  inflating: train_dataset/LCQMC/.DS_Store  \n",
            "  inflating: __MACOSX/train_dataset/LCQMC/._.DS_Store  \n",
            "  inflating: train_dataset/LCQMC/dev.txt  \n",
            "  inflating: __MACOSX/train_dataset/LCQMC/._dev.txt  \n",
            "  inflating: train_dataset/LCQMC/train.txt  \n",
            "  inflating: __MACOSX/train_dataset/LCQMC/._train.txt  \n",
            "  inflating: train_dataset/LCQMC/test.txt  \n",
            "  inflating: __MACOSX/train_dataset/LCQMC/._test.txt  \n",
            "  inflating: train_dataset/OPPO/.DS_Store  \n",
            "  inflating: __MACOSX/train_dataset/OPPO/._.DS_Store  \n",
            "  inflating: train_dataset/OPPO/dev.txt  \n",
            "  inflating: __MACOSX/train_dataset/OPPO/._dev.txt  \n",
            "  inflating: train_dataset/OPPO/train.txt  \n",
            "  inflating: __MACOSX/train_dataset/OPPO/._train.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbpTp_5kpKjb",
        "outputId": "fc59b997-34bd-428c-a46d-ad63e332d389"
      },
      "source": [
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = 'bert-base-chinese'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels = 2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCGKkQ54pUwv"
      },
      "source": [
        "sequences = [\n",
        "    \"婴儿吃什么蔬菜好\",\n",
        "    \"婴儿吃什么绿色蔬菜好\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "batch['labels'] = torch.tensor([1, 1])  # tokenizer出来的结果是一个dictionary，所以可以直接加入新的 key-value\n",
        "\n",
        "##这个label表示两者都是positive的\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss  #这里的 loss 是直接根据 batch 中提供的 labels 来计算的，回忆：前面章节查看 model 的输出的时候，有loss这一项\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADKMlPPNpf1Q"
      },
      "source": [
        "#处理一下data\n",
        "# ROOT = '/Users/lijie/Documents/nlp/data/train_dataset'\n",
        "ROOT = '/content/train_dataset'\n",
        "names = ['BQ','LCQMC','OPPO']\n",
        "# 将其中所有的data处理成一个dict：{'sentence1': 'xxx', 'sentence2': 'yyyy','label':0}\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "#每个数据集中有train.txt, dev.txt, test.txt (OPPO中没有test.txt)\n",
        "def process_data(path = ROOT,names = names):\n",
        "    paths = [Path(path)/Path(name) for name in names]\n",
        "    dataset_dict = {'train':{'sentence1':[],'sentence2':[],'label':[]},\n",
        "                    'validation':{'sentence1':[],'sentence2':[],'label':[]},\n",
        "                    'test':{'sentence1':[],'sentence2':[],'label':[]}}\n",
        "    for p in paths:\n",
        "        for file in [ p/Path(file) for file in os.listdir(p)]:\n",
        "            if 'train.txt' in str(file):\n",
        "                with open(file,'r') as f:\n",
        "                    # print(file)\n",
        "                    for line in f.readlines():\n",
        "                        # print(process_text(line))\n",
        "                        s1,s2,label = process_text(line)\n",
        "                        dataset_dict['train']['sentence1'].append(s1)\n",
        "                        dataset_dict['train']['sentence2'].append(s2)\n",
        "                        dataset_dict['train']['label'].append(int(label))\n",
        "            elif 'dev.txt' in str(file):\n",
        "                with open(file,'r') as f:\n",
        "                    for line in f.readlines():\n",
        "                        # print(process_text(line))\n",
        "                        s1,s2,label = process_text(line)\n",
        "                        dataset_dict['validation']['sentence1'].append(s1)\n",
        "                        dataset_dict['validation']['sentence2'].append(s2)\n",
        "                        dataset_dict['validation']['label'].append(int(label))\n",
        "            elif 'test.txt' in str(file):\n",
        "                with open(file,'r') as f:\n",
        "                    for line in f.readlines():\n",
        "                        s1,s2,label = process_text(line)\n",
        "                        dataset_dict['test']['sentence1'].append(s1)\n",
        "                        dataset_dict['test']['sentence2'].append(s2)\n",
        "                        dataset_dict['test']['label'].append(int(label))\n",
        "    return dataset_dict\n",
        "\n",
        "def process_text(text):\n",
        "    return text.replace('\\n','').split('\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwgkbPoHp9Hq"
      },
      "source": [
        "dataset = process_data()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBYl41PDp_hY"
      },
      "source": [
        "train_encodings = tokenizer(dataset['train']['sentence1'],dataset['train']['sentence2'],truncation=True, padding=True)\n",
        "val_encodings = tokenizer(dataset['validation']['sentence1'],dataset['validation']['sentence2'], truncation=True, padding=True)\n",
        "# test_encodings = tokenizer(dataset['test']['sentence1'],dataset['test']['sentence2'], truncation=True, padding=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFKyOzdUqAoi"
      },
      "source": [
        "train_label = dataset['train']['label']\n",
        "val_label = dataset['validation']['label']\n",
        "test_label = dataset['test']['label']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z77oV9E8qBt_"
      },
      "source": [
        "import torch\n",
        "\n",
        "class DDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DDataset(train_encodings, train_label)\n",
        "val_dataset = DDataset(val_encodings, val_label)\n",
        "# test_dataset = DDataset(test_encodings, test_label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wALJJN5wqDGx"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L_-U69uGqEji",
        "outputId": "b8baa901-ef26-4b79-8a82-dc190cb4707d"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 505939\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 94866\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='571' max='94866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  571/94866 03:25 < 9:27:05, 2.77 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.722100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.690100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.678900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.701400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.584900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.530200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.623700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.489900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.542200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.509300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.454700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.556700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.596400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.497800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.574500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.525500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.519000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.482200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.523500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.522300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.457500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.526100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.447200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.463500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.471800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.420900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.528300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.532700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKeC-JcqFn6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}